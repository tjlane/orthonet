#!/usr/bin/env python

import os
import argparse
import time
import stat
import numpy as np
from os.path import join as pjoin

import torch
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.multiprocessing as mp
import torch.distributed as dist


from orthonet import visual
from orthonet import jacob
from orthonet.reducer import Reducer
from orthonet.loaders import load_bot, load_mnist, load_dsprites
from orthonet.models import AE, VAE, SpritesVAE, SpritesVAEbn, MnistVAE

import wandb


def file_age(pathname):
    age_in_sec = time.time() - os.stat(pathname)[stat.ST_MTIME]
    return age_in_sec / 3600.0


def parse_args():

    parser = argparse.ArgumentParser(description='run VAE on simple simulation')
    parser.add_argument('data_file', type=str, help='data file to load')
    parser.add_argument('--model-type', type=str, 
                        choices=['ae', 'vae', 'spritesvae', 'spritesvaebn', 'mnistvae'],
                        required=True, default='ae',
                        help='architecture')
    parser.add_argument('--ngpu', type=int, default=1,
                        help='number of GPUs to ask for')
    parser.add_argument('--batch-size', type=int, default=32,
                        help='input batch size for training (default: 32)')
    parser.add_argument('--epochs', type=int, default=250,
                        help='number of epochs to train (default: 250)')
    parser.add_argument('--latent-size', type=int, default=5,
                        help='size of latent layer (default: 5)')
    parser.add_argument('--vaebeta', type=float, default=1.0,
                        help='vae beta parameter (default: 1.0)')
    parser.add_argument('--obeta', type=float, default=1.0,
                        help='ortho beta parameter (default: 1.0)')
    parser.add_argument('--dbeta', type=float, default=1.0,
                        help='diagn beta parameter (default: 1.0)')
    parser.add_argument('--no-jac', action='store_true', default=False,
                        help='hard disable of jacobian computation')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--use-wandb', action='store_true', default=False,
                        help='activate wandb monitoring')
    args = parser.parse_args()

    return args


def fwd_pass(model, data):

    output = model(data) # output = (recon, mu, logvar)

    bce = model.loss_function(data, *output)
    y = output[0]
    z = output[1]

    if model.no_jac:
        jgl = torch.tensor(0.0)
    else:
        jgl = model.ortho_beta * jacob.jg_loss(model.decode, z, model.input_size,
                                               diagonal_weight=model.diagn_beta,
                                               reduction='sum')

    return bce, jgl, z, y


def train(epoch, train_loader, model, device, use_wandb=False):

    lr = 1e-3
    #lr = 4e-3 / (2 ** epoch)
    #print('learning rate --> %e' % lr)

    optimizer = torch.optim.Adam(model.parameters(), 
                                 lr=lr, 
                                 weight_decay=1e-4)

    model.train()

    train_loss = 0.0
    bce_loss   = 0.0
    jgl_loss   = 0.0
    
    for batch_idx, batch in enumerate(train_loader):

        start_time = time.perf_counter()

        if type(batch) is tuple: 
            data = batch[0].squeeze()
        else:
            data = batch

        if batch.size(0) == 0:
            continue

        data = data.cuda(non_blocking=True)
        
        bce, jgl, _, _ = fwd_pass(model, data)

        bce_loss   += bce.item()
        jgl_loss   += jgl.item()
        train_loss += bce.item() + jgl.item()

        loss = bce + jgl

        optimizer.zero_grad()
        loss.backward()

        # average gradients across processes
        # may want to do this infrequently if slow
        if hasattr(model, 'reducer'):
            model.reducer.reduce()

        optimizer.step()

        finish_time = time.perf_counter()
        rate = len(data) / (finish_time - start_time)

        if (device == 0) and use_wandb and (batch_idx % 10 == 0):
            wandb.log({'epoch'       : epoch,
                       'train_loss'  : loss.item() / len(data),
                       'bce_loss'    : bce.item()  / len(data),
                       'jgl_loss'    : jgl.item()  / len(data),
                       'proc_rate'   : rate
                       })

        if batch_idx % 10 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\t'
                  'Loss: {:.6f} | BCE: {:.2e} | JAC: {:.2e} | {:.1f} Hz'.format(
                    epoch, 
                    batch_idx * len(data), 
                    len(train_loader.dataset),
                    100. * batch_idx / len(train_loader),
                    loss.item() / len(data),
                    bce.item()  / len(data), 
                    jgl.item()  / len(data),
                    rate ))


    avg_loss = train_loss / len(train_loader.dataset)
    bce_avg = bce_loss    / len(train_loader.dataset)
    jgl_avg = jgl_loss    / len(train_loader.dataset)
    if (device == 0) and use_wandb:
        wandb.log({'epoch'       : epoch,
                   'train_loss'  : loss.item() / len(data),
                   'bce_loss'    : bce.item()  / len(data),
                   'jgl_loss'    : jgl.item()  / len(data),
                   'proc_rate'   : rate
                   })

    print('====> Epoch: {} Average loss: {:.4f}'.format(
          epoch, avg_loss))

    if (device == 0) and use_wandb:
        wandb.log({'epoch'       : epoch,
                   'train_Eavg'  : avg_loss,
                   'bce_Eavg'    : bce_avg,
                   'jgl_Eavg'    : jgl_avg
                   })

    return avg_loss, bce_avg, jgl_avg


def test(epoch, test_loader, model, device, use_wandb=False):

    model.eval()
    test_loss = 0.0
    test_bce  = 0.0
    test_jgl  = 0.0

    with torch.no_grad():

        for i, batch in enumerate(test_loader):

            if type(batch) is tuple:
                data = batch[0].squeeze()
            else:
                data = batch

            data = data.cuda(non_blocking=True)
            bce, jgl, z, y = fwd_pass(model, data)

            test_bce  += bce.item()
            test_jgl  += jgl.item()
            test_loss += bce.item() + jgl.item()

    avg_loss = test_loss / len(test_loader.dataset)
    print('====> Test set loss: {:.4f} | BCE: {:.2e} | JAC: {:.2e}'.format(
           avg_loss, 
           test_bce / len(test_loader.dataset),
           test_jgl / len(test_loader.dataset) ))

    if (device == 0) and use_wandb:
        wandb.log({'epoch'     : epoch,
                   'test_loss' : avg_loss})

        y_cpu = y.cpu()
        n_samples = min(y_cpu.size(0), 8)
        wandb.log({'samples' : [wandb.Image(y_cpu[j], caption="recon%d" % j) for j in range(n_samples)]})

    return avg_loss


def main(device, args):
    """
    device = int, rank on node
    """

    if (device == 0) and args.use_wandb:
        wandb.init(project='ortho')  

    # Explicitly setting seed to make sure that 
    # models created in two processes start
    # from same random weights and biases.
    torch.manual_seed(args.seed)

    torch.cuda.set_device(device)

    if args.distributed:

        if args.nodes == 1:
            rank = device
        else:
            raise NotImplementedError('havent done multi node yet')

        dist.init_process_group(                                   
            backend='nccl',                                         
            init_method='env://',                                   
            world_size=args.world_size,
            rank=rank                                               
        )   

    # -------------------
    # data loading


    # if we are running on many GPUs, distribute data
    if args.distributed:
        loader_kwargs = {'num_workers': 0, 
                         'pin_memory' : True,
                         'shuffle'    : False,
                         'drop_last'  : True,
                         'preload'    : False}
    else:
        loader_kwargs = {'num_workers': 1, 
                         'pin_memory' : True,
                         'shuffle' :    True}


    if args.data_file == 'mnist':
        ll = load_mnist(args.batch_size, 
                        loader_kwargs=loader_kwargs)

    elif args.data_file == 'sprites':
        ll = load_dsprites(args.batch_size,
                           device, # rank
                           args.world_size,
                           distributed=args.distributed,
                           loader_kwargs=loader_kwargs)

    else:
        ll = load_bot(args.data_file, 
                      args.batch_size,
                      loader_kwargs=loader_kwargs)


    train_loader, test_loader, data_shape = ll # unpack

    # -------------------
    # model selection

    field_shape = data_shape[1:]
    input_size  = np.product(field_shape)
    latent_size = args.latent_size

    if args.model_type == 'ae':
        model = AE(input_size, latent_size)
    elif args.model_type == 'vae':
        model = VAE(field_shape, latent_size, beta=args.vaebeta)
    elif args.model_type == 'spritesvae':
        model = SpritesVAE(field_shape, latent_size, beta=args.vaebeta)
    elif args.model_type == 'spritesvaebn':
        model = SpritesVAEbn(field_shape, latent_size, beta=args.vaebeta)
    elif args.model_type == 'mnistvae':
        model = MnistVAE(field_shape, latent_size, beta=args.vaebeta)

    model.cuda()
    if args.distributed:
        reducer = Reducer(model)
        model.reducer = reducer # just to keep them together

    if (device == 0) and args.use_wandb:
        wandb.watch(model)

    # --------------------
    # pass on parameters by ducktyping
    model.ortho_beta = args.obeta
    model.diagn_beta = args.dbeta
    model.no_jac     = args.no_jac

    if device == 0:
        print('')
        print('VAE   beta --> ', args.vaebeta)
        print('ortho beta --> ', args.obeta)
        print('diagn beta --> ', args.dbeta)
        print('jacobian off:',   args.no_jac)

    # -------------------
    # main training loop

    train_loss  = np.zeros((args.epochs, 3))
    test_loss   = np.zeros(args.epochs)

    start_time = time.time()
    for epoch in range(1, args.epochs + 1):
        train_loss[epoch-1,:]  = train(epoch, train_loader, model, device, args.use_wandb)
        test_loss[epoch-1]     = test(epoch, test_loader,  model, device, args.use_wandb)

        if device == 0:
            torch.save(model, pjoin(args.resdir, 'checkpoints/model_%d.pt' % epoch))

    print('>>>> Training Time: %.0f seconds' % (time.time() - start_time))

    # -------------------
    # final bookkeeping

    if device == 0:

        visual.save_latent_traversals(args.resdir, model, args.latent_size, field_shape)
        visual.plot_loss_curves(train_loss, test_loss, 
                                save=pjoin(args.resdir, 'loss_curves.png'))

        # save final model
        torch.save(model.cpu().state_dict(), 
                   pjoin(args.resdir, 'model_dict.pt'))

    if args.distributed:
        dist.destroy_process_group()

    return


if __name__ == "__main__":

    args = parse_args()
    args.seed = np.random.randint(999999)
    args.cuda = not args.no_cuda and torch.cuda.is_available()

    # -------------------
    # setup output directory

    resdir = 'ortho_%s-%.2e_ob%.2e_db%.2e' % (args.model_type, 
                                              args.vaebeta, 
                                              args.obeta, 
                                              args.dbeta)
    args.resdir = resdir

    for dirc in [resdir,
                 pjoin(resdir, 'reconstructions'),
                 pjoin(resdir, 'samples'),
                 pjoin(resdir, 'checkpoints')]:

        if not os.path.exists(dirc):
            os.mkdir(dirc)

    if args.data_file == 'sprites':
        if not os.path.exists('/scratch/tjlane'):
            os.mkdir('/scratch/tjlane')

        r = 0
        if not os.path.exists('/scratch/tjlane/dsprites.h5'):
            print('\ncopying sprites data to scratch...')
            r = os.system('cp /u/xl/tjlane/ortho/sprites/dsprites.h5 /scratch/tjlane/dsprites.h5')
        elif (file_age('/scratch/tjlane/dsprites.h5') > 24.0): # older than 1 day
            print('\ncopying sprites data to scratch...')
            r = os.system('cp /u/xl/tjlane/ortho/sprites/dsprites.h5 /scratch/tjlane/dsprites.h5')
        if r != 0:
            raise IOError('copy failed')
    
    # --------------------------
    # device setup  

    # >>> PyTorch Distributed (multiGPU)
    if (args.ngpu > 1) and (not args.no_cuda):

        args.distributed = True
        args.nodes = 1 # for now
        args.world_size = args.ngpu * args.nodes
        
        print('\n-------- GPU PARALLELISM >>')
        print(' >> %d nodes' % args.nodes)
        print(' >> %d gpu/node' % args.ngpu)
        print('')

        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '8899'

        mp.spawn(main, nprocs=args.ngpu, args=(args,), join=True)

    # >>> one GPU or CPU
    else:
        args.distributed = False
        args.world_size = 1
        device_type = "cuda" if args.cuda else "cpu"
        print("Device type: %s" % device_type)
        main(0, args)

