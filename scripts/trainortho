#!/usr/bin/env python

import os
import argparse
import time
import numpy as np
from os.path import join as pjoin

import torch
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.multiprocessing as mp
import torch.distributed as dist

from orthonet import visual
from orthonet import jacob
from orthonet.loaders import load_data, load_mnist
from orthonet.models import AE, VAE, SpritesVAE, MnistVAE, VisDDP

# fix random seeds for debugging
#print('WARNING: random seed fixed...')
#torch.manual_seed(0)
#np.random.seed(0)
#torch.backends.cudnn.deterministic = True
#torch.backends.cudnn.benchmark = False


def parse_args():

    parser = argparse.ArgumentParser(description='run VAE on simple simulation')
    parser.add_argument('data_file', type=str, help='data file to load')
    parser.add_argument('--model-type', type=str, 
                        choices=['ae', 'vae', 'spritesvae', 'mnistvae'],
                        required=True, default='ae',
                        help='architecture')
    parser.add_argument('--ngpu', type=int, default=1,
                        help='number of GPUs to ask for')
    parser.add_argument('--batch-size', type=int, default=32,
                        help='input batch size for training (default: 32)')
    parser.add_argument('--epochs', type=int, default=250,
                        help='number of epochs to train (default: 250)')
    parser.add_argument('--latent-size', type=int, default=5,
                        help='size of latent layer (default: 5)')
    parser.add_argument('--vaebeta', type=float, default=1.0,
                        help='vae beta parameter (default: 1.0)')
    parser.add_argument('--obeta', type=float, default=1.0,
                        help='ortho beta parameter (default: 1.0)')
    parser.add_argument('--dbeta', type=float, default=1.0,
                        help='diagn beta parameter (default: 1.0)')
    parser.add_argument('--no-jac', action='store_true', default=False,
                        help='hard disable of jacobian computation')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='enables CUDA training')
    args = parser.parse_args()

    return args


def fwd_pass(model, data):

    # dblchk
    z = model.encode(data)
    y = model(data)

    # vae model returns mu, var -- handle that
    if type(y) is not tuple:
        bce = model.loss_function(data, y)
    else:
        bce = model.loss_function(data, *y)
        y = y[0]

    if model.no_jac:
        jgl = torch.tensor(0.0)
    else:
        jgl = model.ortho_beta * jacob.jg_loss(model.decode, z, model.input_size,
                                               diagonal_weight=model.diagn_beta,
                                               reduction='sum')
    return bce, jgl, z, y


def train(epoch, train_loader, model, device):

    lr = 1e-3
    if epoch > 30:
        lr = 1e-4

    optimizer = torch.optim.Adam(model.parameters(), 
                                 lr=lr, 
                                 weight_decay=1e-3)

    model.train()
    train_loss = 0.0
    bce_loss   = 0.0
    jgl_loss   = 0.0

    for batch_idx, batch in enumerate(train_loader):

        if type(batch) is tuple: 
            data = batch[0].squeeze()
        else:
            data = batch

        data = data.cuda(non_blocking=True)
        bce, jgl, _, _ = fwd_pass(model, data)

        bce_loss += bce.item()
        jgl_loss += jgl.item()
        train_loss += bce.item() + jgl.item()

        loss = bce + jgl

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 10 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\t'
                  'Loss: {:.6f} | BCE: {:.2e} | JAC: {:.2e}'.format(
                    epoch, 
                    batch_idx * len(data), 
                    len(train_loader.dataset),
                    100. * batch_idx / len(train_loader),
                    loss.item() / len(data),
                    bce.item() / len(data), 
                    jgl.item() / len(data) ))

    avg_loss = train_loss / len(train_loader.dataset)
    print('====> Epoch: {} Average loss: {:.4f}'.format(
          epoch, avg_loss))

    bce_avg = bce_loss / len(train_loader.dataset)
    jgl_avg = jgl_loss / len(train_loader.dataset)

    return avg_loss, bce_avg, jgl_avg


def test(epoch, test_loader, model, device):

    model.eval()
    test_loss = 0.0
    test_bce  = 0.0
    test_jgl  = 0.0

    with torch.no_grad():

        for i, batch in enumerate(test_loader):

            if type(batch) is tuple:
                data = batch[0].squeeze()
            else:
                data = batch

            data = data.cuda(non_blocking=True)
            bce, jgl, z, y = fwd_pass(model, data)

            test_bce += bce.item()
            test_jgl += jgl.item()
            test_loss += bce.item() + jgl.item()

    avg_loss = test_loss / len(test_loader.dataset)
    print('====> Test set loss: {:.4f} | BCE: {:.2e} | JAC: {:.2e}'.format(
           avg_loss, 
           test_bce / len(test_loader.dataset),
           test_jgl / len(test_loader.dataset) ))


    return avg_loss


def main(device, args):
    """
    device = int, rank on node
    """

    # Explicitly setting seed to make sure that 
    # models created in two processes start
    # from same random weights and biases.
    torch.manual_seed(args.seed)

    torch.cuda.set_device(device)

    if args.distributed:

        if args.nodes == 1:
            rank = device
        else:
            raise NotImplementedError('havent done multi node yet')

        dist.init_process_group(                                   
            backend='nccl',                                         
            init_method='env://',                                   
            world_size=args.world_size,
            rank=rank                                               
        )   

    # -------------------
    # data loading


    # if we are running on many GPUs, distribute data
    if args.distributed:
        loader_kwargs = {'num_workers': 0, 
                         'pin_memory' : True,
                         'shuffle'    : False}
    else:
        loader_kwargs = {'num_workers': 2, 
                         'pin_memory' : True,
                         'shuffle' :    True}


    if args.data_file == 'mnist':
        ll = load_mnist(args.batch_size, 
                        loader_kwargs=loader_kwargs)

    elif args.data_file == 'sprites':
        ll = load_dsprites(args.batch_size,
                           device, # rank
                           args.world_size,
                           preload=False)

    else:
        ll = load_bot(args.data_file, 
                      args.batch_size,
                      loader_kwargs=loader_kwargs)


    train_loader, test_loader, data_shape = ll # unpack

    # -------------------
    # model selection

    field_shape = data_shape[1:]
    input_size  = np.product(field_shape)
    latent_size = args.latent_size

    if args.model_type == 'ae':
        model = AE(input_size, latent_size)
    elif args.model_type == 'vae':
        model = VAE(field_shape, latent_size, beta=args.vaebeta)
    elif args.model_type == 'spritesvae':
        model = SpritesVAE(field_shape, latent_size, beta=args.vaebeta)
    elif args.model_type == 'mnistvae':
        model = MnistVAE(field_shape, latent_size, beta=args.vaebeta)

    model.cuda()
    if args.distributed:
        model = VisDDP(model, device_ids=[device])


    # --------------------
    # pass on parameters by ducktyping
    model.ortho_beta = args.obeta
    model.diagn_beta = args.dbeta
    model.no_jac     = args.no_jac

    print('VAE   beta --> ', args.vaebeta)
    print('ortho beta --> ', args.obeta)
    print('diagn beta --> ', args.dbeta)
    print('jacobian off:',   args.no_jac)

    # -------------------
    # main training loop

    train_loss  = np.zeros((args.epochs, 3))
    test_loss   = np.zeros(args.epochs)

    start_time = time.time()
    for epoch in range(1, args.epochs + 1):
        train_loss[epoch-1,:]  = train(epoch, train_loader, model, device)
        test_loss[epoch-1]     = test(epoch, test_loader,  model, device)
        if device == 0:
            torch.save(model, pjoin(args.resdir, 'checkpoints/model_%d.pt' % epoch))

    print('>>>> Training Time: %.0f seconds' % (time.time() - start_time))

    # -------------------
    # final bookkeeping

    if device == 0:

        visual.save_latent_traversals(args.resdir, model, args.latent_size, field_shape)
        visual.plot_loss_curves(train_loss, test_loss, 
                                save=pjoin(args.resdir, 'loss_curves.png'))

        # save final model
        torch.save(model.cpu().state_dict(), 
                   pjoin(args.resdir, 'model_dict.pt'))

    if args.distributed:
        dist.destroy_process_group()

    return


if __name__ == "__main__":

    args = parse_args()
    args.seed = np.random.randint(999999)
    args.cuda = not args.no_cuda and torch.cuda.is_available()

    # -------------------
    # setup output directory

    resdir = 'ortho_%s-%.2e_ob%.2e_db%.2e' % (args.model_type, 
                                              args.vaebeta, 
                                              args.obeta, 
                                              args.dbeta)
    args.resdir = resdir

    for dirc in [resdir,
                 pjoin(resdir, 'reconstructions'),
                 pjoin(resdir, 'samples'),
                 pjoin(resdir, 'checkpoints')]:

        if not os.path.exists(dirc):
            os.mkdir(dirc)

    if args.data_file == 'sprites':
        if not os.path.exists('/scratch/tjlane'):
            os.mkdir('/scratch/tjlane')

        #if not os.path.exists('/scratch/tjlane/dsprites.h5'):
        print('\ncopying sprites data to scratch...')
        r = os.system('cp /u/xl/tjlane/ortho/sprites/dsprites.h5 /scratch/tjlane/dsprites.h5')
        print('... done.\n')

    # --------------------------
    # device setup  

    # >>> PyTorch Distributed (multiGPU)
    if (args.ngpu > 1) and (not args.no_cuda):

        args.distributed = True
        args.nodes = 1 # for now
        args.world_size = args.ngpu * args.nodes
        
        print('\n-------- GPU PARALLELISM >>')
        print(' >> %d nodes' % args.nodes)
        print(' >> %d gpu/node' % args.ngpu)
        print('')

        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '8899'

        mp.spawn(main, nprocs=args.ngpu, args=(args,), join=True)

    # >>> one GPU or CPU
    else:
        args.distributed = False
        device_type = "cuda" if args.cuda else "cpu"
        print("Device type: %s" % device_type)
        main(0, args)

