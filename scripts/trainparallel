#!/usr/bin/env python

import os
import h5py
import argparse
import time
import numpy as np
from os.path import join as pjoin

import torch
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn import functional as F

from torchvision import datasets, transforms
from torchvision.utils import save_image

from orthonet import visual
from orthonet import jacob
from orthonet.models import AE, VAE, SpritesVAE, MnistVAE, VisDistDataParallel

# fix random seeds for debugging
#print('WARNING: random seed fixed...')
#torch.manual_seed(0)
#np.random.seed(0)
#torch.backends.cudnn.deterministic = True
#torch.backends.cudnn.benchmark = False


def parse_args():

    parser = argparse.ArgumentParser(description='run VAE on simple simulation')
    parser.add_argument('data_file', type=str, help='data file to load')
    parser.add_argument('--model-type', type=str, choices=['ae', 'vae', 'spritesvae', 'mnistvae'],
                        required=True, default='ae',
                        help='architecture')
    parser.add_argument('--ngpu', type=int, default=1,
                        help='number of GPUs to ask for')
    parser.add_argument('--batch-size', type=int, default=32,
                        help='input batch size for training (default: 32)')
    parser.add_argument('--epochs', type=int, default=250,
                        help='number of epochs to train (default: 250)')
    parser.add_argument('--latent-size', type=int, default=5,
                        help='size of latent layer (default: 5)')
    parser.add_argument('--vaebeta', type=float, default=1.0,
                        help='vae beta parameter (default: 1.0)')
    parser.add_argument('--obeta', type=float, default=1.0,
                        help='ortho beta parameter (default: 1.0)')
    parser.add_argument('--dbeta', type=float, default=1.0,
                        help='diagn beta parameter (default: 1.0)')
    parser.add_argument('--no-jac', action='store_true', default=False,
                        help='hard disable of jacobian computation')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='enables CUDA training')
    args = parser.parse_args()

    return args


def setup_device(args):

    args.cuda = not args.no_cuda and torch.cuda.is_available()
    device_type = "cuda:0" if args.cuda else "cpu"
    device = torch.device(device_type)
    print("Device: %s" % device_type)

    loader_kwargs = {'num_workers': args.ngpu, 'pin_memory': True} if args.cuda else {}

    return device, loader_kwargs


def load_data(data_file, batch_size, loader_kwargs={}, traintest_split=0.9):

    f = h5py.File(data_file, 'r', libver='latest', swmr=True)

    if 'data' in f.keys():   # my toy models
        data = np.array(f['data']).astype(np.float32)
        data = data.clip(0.0, 1.0)
    elif 'imgs' in f.keys(): # dsprites
        cut = 32768
        data = np.array(f['imgs'][:cut]).astype(np.float32)
        pi = np.random.permutation(data.shape[0])
        data = data[pi,:,:]

    print('shp:', data.shape)
    f.close()

    split = int(data.shape[0] * traintest_split)
    size  = data.shape[0]

    print('\tTrain/Test: %d/%d' % (split, size-split))

    train_data = torch.stack([torch.tensor(data[i]) for i in range(0, split)])
    test_data  = torch.stack([torch.tensor(data[i]) for i in range(split, size)])

    train_loader = torch.utils.data.DataLoader( 
            torch.utils.data.TensorDataset(train_data),
        batch_size=batch_size, shuffle=True, **loader_kwargs)

    test_loader = torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(test_data),
        batch_size=batch_size, shuffle=True, **loader_kwargs)

    return train_loader, test_loader, data.shape


def load_mnist(batch_size, loader_kwargs={}):

    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('./data', train=True, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor(),
                       ])),
        batch_size=batch_size, shuffle=True, **loader_kwargs)

    test_loader = torch.utils.data.DataLoader(
        datasets.MNIST('./data', train=False, transform=transforms.Compose([
                           transforms.ToTensor(),
                       ])),
        batch_size=batch_size, shuffle=True, **loader_kwargs)  

    # dataset is 60 000 training and 10 000 test 28x28 images
    data_shape = (60000 + 10000, 28, 28)

    return train_loader, test_loader, data_shape

# ---------------------------------------------------------------------------------


def save_comparison(epoch, x, y, batch_size):
    n = min(x.shape[0], 8)
    comp_shp = (min(batch_size, x.shape[0]), 1, *x.shape[1:])
    comparison = torch.cat([x.view(*comp_shp)[:n],
                            y.view(*comp_shp)[:n]])
    save_image(comparison.cpu(),
               pjoin(resdir, 'reconstructions/reconstruction_%d.png' % epoch), nrow=n)
    return


def fwd_pass(model, data):

    # dblchk
    z = model.encode(data)
    y = model(data)

    # vae model returns mu, var -- handle that
    if type(y) is not tuple:
        bce = model.loss_function(data, y)
    else:
        bce = model.loss_function(data, *y)
        y = y[0]

    if NO_JAC:
        jgl = torch.tensor(0.0)
    else:
        jgl = model.ortho_beta * jacob.jg_loss(model.decode, z, model.input_size,
                                               diagonal_weight=model.diagn_beta,
                                               reduction='sum')
    return bce, jgl, z, y


def train(epoch, train_loader, model):

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)

    model.train()
    train_loss = 0.0
    bce_loss   = 0.0
    jgl_loss   = 0.0

    for batch_idx, batch in enumerate(train_loader):

        data = batch[0].squeeze()

        data = data.to(device, non_blocking=True)
        bce, jgl, _, _ = fwd_pass(model, data)

        bce_loss += bce.item()
        jgl_loss += jgl.item()
        train_loss += bce.item() + jgl.item()

        loss = bce + jgl

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 10 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f} | BCE: {:.2e} | JAC: {:.2e}'.format(
                epoch, 
                batch_idx * len(data), 
                len(train_loader.dataset),
                100. * batch_idx / len(train_loader),
                loss / len(data),
                bce.item() / len(data), 
                jgl.item() / len(data) ))

    avg_loss = train_loss / len(train_loader.dataset)
    print('====> Epoch: {} Average loss: {:.4f}'.format(
          epoch, avg_loss))

    bce_avg = bce_loss / len(train_loader.dataset)
    jgl_avg = jgl_loss / len(train_loader.dataset)

    return avg_loss, bce_avg, jgl_avg


def test(epoch, test_loader, model):

    model.eval()
    test_loss = 0.0
    test_bce  = 0.0
    test_jgl  = 0.0

    with torch.no_grad():

        for i, batch in enumerate(test_loader):

            data = batch[0].squeeze()

            data = data.to(device, non_blocking=True)
            bce, jgl, z, y = fwd_pass(model, data)

            test_bce += bce.item()
            test_jgl += jgl.item()
            test_loss += bce.item() + jgl.item()

        save_comparison(epoch, data, y, test_loader.batch_size) 

    test_loss /= len(test_loader.dataset)
    print('====> Test set loss: {:.4f} | BCE: {:.2e} | JAC: {:.2e}'.format(
           test_loss, 
           test_bce / len(test_loader.dataset),
           test_jgl / len(test_loader.dataset) ))


    return test_loss


def main(rank, world_size):

    args = parse_args()
    parallel_setup(rank, world_size)

    print('VAE   beta --> ', args.vaebeta)
    print('ortho beta --> ', args.obeta)
    print('diagn beta --> ', args.dbeta)

    global NO_JAC
    NO_JAC = args.no_jac

    global device
    device, loader_kwargs = setup_device(args)

    if args.data_file == 'mnist':
        train_loader, test_loader, data_shape = load_mnist(args.batch_size, 
                                                           loader_kwargs=loader_kwargs)
    else:
        train_loader, test_loader, data_shape = load_data(args.data_file, 
                                                          args.batch_size, 
                                                          loader_kwargs=loader_kwargs)

    global resdir
    resdir = 'ortho_%s-%.2e_ob%.2e_db%.2e' % (args.model_type, args.vaebeta, args.obeta, args.dbeta)
    for dirc in [resdir, 
                 pjoin(resdir, 'reconstructions'), 
                 pjoin(resdir, 'samples'),
                 pjoin(resdir, 'checkpoints')]:
        if not os.path.exists(dirc):
            os.mkdir(dirc)


    # -------------------

    field_shape = data_shape[1:]
    input_size  = np.product(field_shape)
    latent_size = args.latent_size

    if args.model_type == 'ae':
        model = AE(input_size, latent_size)
    elif args.model_type == 'vae':
        model = VAE(field_shape, latent_size, beta=args.vaebeta)
    elif args.model_type == 'spritesvae':
        model = SpritesVAE(field_shape, latent_size, beta=args.vaebeta)
    elif args.model_type == 'mnistvae':
        model = MnistVAE(field_shape, latent_size, beta=args.vaebeta)

    if (args.ngpu > 1) and (not args.no_cuda):

        n_to_use = min(torch.cuda.device_count(), args.ngpu)

        print('\n-------- GPU PARALLELISM >>')
        print(' >> requesting %d GPUs' % args.ngpu)
        print(' >> %d available' % torch.cuda.device_count())
        print(' >> using %d GPUs' % n_to_use)
        print('')

        device_ids = range(0, n_to_use)
        model.to(rank)
        model = VisDistDataParallel(model, device_ids=device_ids, output_device=0)
    else:
        model.to(device)

    # pass on parameters by ducktyping
    model.ortho_beta = args.obeta
    model.diagn_beta = args.dbeta

    # -------------------

    train_loss  = np.zeros((args.epochs, 3))
    test_loss   = np.zeros(args.epochs)

    start_time = time.time()
    for epoch in range(1, args.epochs + 1):

        train_loss[epoch-1,:]  = train(epoch, train_loader, model)
        test_loss[epoch-1]     = test(epoch, test_loader,  model)

        torch.save(model, pjoin(resdir, 'checkpoints/model_%d.pt' % epoch))

        with torch.no_grad():

            # generate some random samples
            sample = torch.randn(64, args.latent_size).to(device)
            sample = model.decode(sample).cpu()
            save_image(sample.view(64, 1, *field_shape),
                       pjoin(resdir, 'samples/sample_%d.png' % epoch))

    print('>>>> Training Time: %.0f' % (time.time() - start_time))

    with torch.no_grad():

        # traverse each latent dimension
        n_latent_samples = 12 * 4
        for a in range(args.latent_size):
            sample = np.zeros((n_latent_samples, args.latent_size), dtype=np.float32)
            sample[:,a] = np.linspace(-2, 2, n_latent_samples)
            sample = torch.from_numpy(sample).to(device)
            sample = model.decode(sample).cpu()
            save_image(sample.view(n_latent_samples, 1, *field_shape),
                       pjoin(resdir, 'z' + str(a) + '.png'))

        # save the final model
        torch.save(model.cpu(), pjoin(resdir, 'model.pt'))

    visual.plot_loss_curves(train_loss, test_loss, 
                            save=pjoin(resdir, 'loss_curves.png'))

    return


def parallel_setup(rank, world_size):

    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '8899'

    # initialize the process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)

    # Explicitly setting seed to make sure that models created in two processes
    # start from same random weights and biases.
    torch.manual_seed(42)
    return


def parallel_cleanup():
    dist.destroy_process_group()
    return


if __name__ == "__main__":
    world_size=4 # TODO
    mp.spawn(main, args=(world_size,), nprocs=world_size, join=True)

